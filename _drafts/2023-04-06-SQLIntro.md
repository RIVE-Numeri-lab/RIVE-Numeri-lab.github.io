---
layout: default
category: Programming
thumbnail: duckdb.png
title: "Introduction to relational databases and SQL"
author: "Charles Martin"
date: "2023-04-06"
lang: en
output:
  html_document:
    highlight: haddock
    keep_md: yes
    theme: readable
    toc: yes
    self_contained: true
    toc_float: true	
---

# Introduction to relational databases and SQL
{:.no_toc}

# April 2023
{:.no_toc}

## Charles Martin
{:.no_toc}

* TOC
{:toc}

# What is a database?
At its simplest, a database is a collection of data frames,
which in this jargon are called **tables**. Each of these tables includes
several columns (or variables) that are called **fields** in the vocabulary specific to
databases.

Unlike data frames that *exist* in the computer's memory
tables are stored on the hard drive. They can therefore be much larger in size.

Also, unlike data tables, database tables
are almost always indexed. That is, some work is done upstream, 
as data is added/modified/deleted, to speed up filtering and sorting when they are needed. Note that this feature is not 
unique to databases. For example, the data.table library can also index
its data.

# Data structure and normalization
Unlike data frames which are a fixed product, generated at the end
of the data collection, databases are intended to be dynamic.
They include a whole infrastructure to enable the insertion of new
data, their update, their deletion, etc., in a secure way.

For these changes to go well, how to structure our data
is of particular importance, even more critical than in a data frame. A whole field of study with its own terminology is 
dedicated to this subject: database normalization.

Obviously, we can only skim over these techniques, but it is important to
nevertheless understand *the* main principle, which is to **never duplicate information**.

For example, this data frame is perfect for R:

|parcelle|site|richesse_parcelle|ph_site|
|:-:|:-:|:-:|:-:|
|A|X|3|7.5|
|B|X|2|7.5|
|C|X|5|7.5|
|D|Y|1|6.9|
|E|Y|0|6.9|
|F|Y|3|6.9|

But it would not be appropriated in a relational database environment, in which
it should be replaced by the following tables : 

**sites**

|id|nom|ph|
|:-:|:-:|:-:|
|1|X|7.5
|2|Y|6.9

**parcelles**

|id|site_id|nom|richesse|
|:-:|:-:|:-:|:-:|
|1|1|A|3|
|2|1|B|2|
|3|1|C|5|
|4|2|D|1|
|5|2|E|0|
|6|2|F|3|

Notice in each table the presence of an **id** column, which becomes the unique identifierfor each observation. This identifier is called the **primary key**
of the table.

The link between the two tables is specified by the column **site_id**,
which is called in this jargon the **foreign key**.

So the relationship here is a **1:N** relationship, a one-to-many relationship. A
site can have multiple parcels, but each parcel belongs only to
a single site.

There are several other types of relationships, for example **N:N** relationships, many-to-many. This type of relationship would be appropriate to store the list
species present in each plot:

**especes**

|id|nom|
|:-:|:-:|
|1|perchaude|
|2|meunier|

**presences**

|parcelle_id|espece_id|
|:-:|:-:|
|1|1|
|1|3|

Note that to define an N:N relationship, you must create an intermediate table,
which contains only identifiers associating the two tables. It's the role the presences table plays here, connecting the species table and the plots table
in an N:N relationship.


# Overview of the available software packages

There are dozens, if not hundreds, of relation database software packages. They can generally be divided into three
categories of use:

* Huge databases in the *cloud*, like Google BigQuery or Amazon Redshift.
These solutions are extremely powerful because they can tap into a
huge *pool* of resources when performing requests. Their usage
is also very expensive.

* Database servers, like MySQL, PostgreSQL, SQL Server, etc. These
solutions allow simultaneous querying of their data by multiple users
at once. They are an excellent client-server solution when several
users must play simultaneously with the same data.

* Embedded databases, such as SQLite and DuckDB. These solutions
run directly on your computer, greatly increasing its ability to
*crunch* data, while avoiding the configuration of a server infrastructure.

For this workshop, we will be using an on-board database,
 DuckDB. The advantage of this database for learning situations
is that it is erased and recreated each time R is started. It is
also extremely fast, being able to handle huge queries, often
faster than its competitor SQLite.

However, almost everything we will see could be transferred
to another software without problem.


# DuckDB workspace preparation

```r
library(tidyverse)
```

```
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
✔ ggplot2 3.4.0      ✔ purrr   1.0.1 
✔ tibble  3.1.8      ✔ dplyr   1.0.10
✔ tidyr   1.3.0      ✔ stringr 1.5.0 
✔ readr   2.1.2      ✔ forcats 0.5.2 
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
```

```r
library(DBI)
library(duckdb)
```

All our operations on databases will go through the DBI library
(DataBase Interface). It will take care of transferring our requests
to the database. On the other hand, as each database has its own
small particularities, we will have to specify at the time of the connection
which database we are connecting to. In our case, it will be
to DuckDB, so:

```r
connexion <- DBI::dbConnect(duckdb::duckdb())
```

If our database was a PostgreSQL server, we could have built our connection like this : 

```r
connexion <- DBI::dbConnect(
  RPostgres::Postgres(),
  hostname = "adresse.com",
  port = 1234
)
```

Finally, in a real analytics project, it would probably have been appropriate
to specify a folder in which DuckDB can store the database
rather than having to recreate it each time you start R:

```r
connexion <- DBI::dbConnect(duckdb::duckdb(), dbdir = "donnees_projet")
```

We will now create some tables and populate them 
to be able to start working. You can also download the dataset
associated with this workshop [here](/assets/SQLIntro_files/FakeData.zip).

Note that the data
are already well organized for import: each observation is labeled with a unique identifier and database structure has already
been normalized.

First, here's how to create a DuckDB table from a CSV file:

```r
duckdb_read_csv(connexion, "especes", "FakeData/especes.csv")
```

Since we have many tables to create, we could automated this process
by using the map function :

```r
liste_tables <- c("especes","sites","parcelles","presences")

map(liste_tables, function(x){
  duckdb_read_csv(connexion, x, str_c("FakeData/",x,".csv"))
})
```

```
[[1]]
[1] 166

[[2]]
[1] 26

[[3]]
[1] 1300000

[[4]]
[1] 1e+07
```

We can then verify that all our tables have been succesfully created.

```r
dbListTables(connexion)
```

```
[1] "especes"   "parcelles" "presences" "sites"    
```

The database with which we are going to work contains 26 sites,
in which a total of 1,300,000 plots have been inventoried. In each
plot, the list of all species (83 in total) were noted,
for a total of 10,000,000 presences. These data are made up for the workshop,
but will allow us to test the speed and power of the databse approach. Don't worry about the execution speed.
On my personal laptop, the whole workshop runs in less than 30 seconds...

Here, for information, is the list of all the fields in each of the tables that
we created:

```r
dbListFields(connexion,"especes")
```

```
[1] "id"  "nom"
```

```r
dbListFields(connexion,"sites")
```

```
[1] "id"  "nom" "ph" 
```

```r
dbListFields(connexion,"parcelles")
```

```
[1] "id"          "site_id"     "nom"         "couvert_pct"
```

```r
dbListFields(connexion,"presences")
```

```
[1] "parcelle_id" "espece_id"  
```
Note that each of the plots is associated with a site by the site_id column,
which makes it a 1:N relationship (a site contains several parcels,
but each parcel only belongs to one site).

The presences table only contains identifiers,
for each of the the species-parcel combinations that have been observed,
describing an N:N relationship.

# Introduction to the SQL language
SQL (Structured Query Language) is a (relatively) standardized computer language for querying databases. I say "relatively" because whereas most of the language is used the same way with each database software, each also adds some specific differences.

SQL defines three main families of commands, those for:

* Extracting data (make queries)
* Defining data (insert, modify, delete, etc.)
* Controling operations (give access, validate transactions, etc.)

In the vast majority of cases, it is possible to connect directly
to the database using a terminal to send our queries.
On the other hand, with the exception of maintenance operations, this way of doing things
is rarely used. Requests are usually sent programmatically
in the language of our choice, here R.

## Queries

To send queries to extract data, we will use the dbGetQuery function from the DBI library. This function expects to receive two things, a connection to a database, and the SQL command to execute.

### Select what, from which table?

The basis of SQL queries is the SELECT command, which allows you to choose
which fields we want to extract from our table, followed by FROM, which specifies
on which table to perform the extraction. For example, to get the same of all sites : 

```r
dbGetQuery(connexion, "SELECT nom FROM sites")
```

```
   nom
1    A
2    B
3    C
4    D
5    E
6    F
7    G
8    H
9    I
10   J
11   K
12   L
13   M
14   N
15   O
16   P
17   Q
18   R
19   S
20   T
21   U
22   V
23   W
24   X
25   Y
26   Z
```

The dbGetQuery function always return a data.frame object, which can be retrieved and used as
a normal R data.frame : 

```r
x <- dbGetQuery(connexion, "SELECT nom FROM sites")

x %>% 
  slice(1:10) %>% 
  arrange(desc(nom))
```

```
   nom
1    J
2    I
3    H
4    G
5    F
6    E
7    D
8    C
9    B
10   A
```

Note that the SELECT and FROM commands have been written here in uppercase. The
majority of database softwares doesn't case if commands are sent in lower or uppercase. The SeLeCt, SELECT and select commands would have the exact same result.
On the other hand, by convention, it is customary to enter the keywords (SELECT, FROM, GROUP BY, etc.)
in uppercase, and the field names in lowercase.

Rather than specifying each of the fields one by one, you can also use an *, which
acts as a wildcard indicating that we want all the fields of this table:


```r
dbGetQuery(connexion, "SELECT * FROM sites")
```

```
   id nom       ph
1   1   A 7.169245
2   2   B 6.954659
3   3   C 6.798699
4   4   D 6.746626
5   5   E 7.241875
6   6   F 6.970178
7   7   G 6.828074
8   8   H 6.692542
9   9   I 7.213349
10 10   J 7.310457
11 11   K 6.945442
12 12   L 7.693323
13 13   M 7.358573
14 14   N 7.196335
15 15   O 6.865845
16 16   P 7.118721
17 17   Q 6.759258
18 18   R 7.125312
19 19   S 7.078200
20 20   T 6.708648
21 21   U 6.577113
22 22   V 7.569592
23 23   W 6.879513
24 24   X 6.655040
25 25   Y 6.991663
26 26   Z 6.876635
```

### Applying filters

Obviously, we don't always want to retrieve everything in a table, we sometimes want
to retrieve only certain information. For example, only the
sites with a pH of 7 or higher:

```r
dbGetQuery(connexion, "SELECT * FROM sites WHERE ph >= 7")
```

```
   id nom       ph
1   1   A 7.169245
2   5   E 7.241875
3   9   I 7.213349
4  10   J 7.310457
5  12   L 7.693323
6  13   M 7.358573
7  14   N 7.196335
8  16   P 7.118721
9  18   R 7.125312
10 19   S 7.078200
11 22   V 7.569592
```

You can also combine many conditions with AND and OR : 

```r
dbGetQuery(connexion, "SELECT * FROM sites WHERE ph >= 7 AND ph < 7.4")
```

```
  id nom       ph
1  1   A 7.169245
2  5   E 7.241875
3  9   I 7.213349
4 10   J 7.310457
5 13   M 7.358573
6 14   N 7.196335
7 16   P 7.118721
8 18   R 7.125312
9 19   S 7.078200
```

### Sorting
You can change the order in which the results will appear by adding
an ORDER BY clause to our query, like this:

```r
dbGetQuery(connexion, "SELECT * FROM sites WHERE ph >= 7 ORDER BY ph")
```

```
   id nom       ph
1  19   S 7.078200
2  16   P 7.118721
3  18   R 7.125312
4   1   A 7.169245
5  14   N 7.196335
6   9   I 7.213349
7   5   E 7.241875
8  10   J 7.310457
9  13   M 7.358573
10 22   V 7.569592
11 12   L 7.693323
```

You can sort by descending values instead of ascending, by adding the DESC modifier : 

```r
dbGetQuery(connexion, "SELECT * FROM sites WHERE ph >= 7 ORDER BY ph DESC")
```

```
   id nom       ph
1  12   L 7.693323
2  22   V 7.569592
3  13   M 7.358573
4  10   J 7.310457
5   5   E 7.241875
6   9   I 7.213349
7  14   N 7.196335
8   1   A 7.169245
9  18   R 7.125312
10 16   P 7.118721
11 19   S 7.078200
```

### Limiter la quantité d'information reçue
Si vous construisez des tableaux de bords, des rapports, etc., il pourrait
vous arriver que vous n'ayez pas besoin de l'ensemble des lignes retournées
par une requête, mais uniquement des premières. On peut utiliser la clause
LIMIT pour couper nos résultats, par exemple en gardant uniquement les 5 premières lignes : 

```r
dbGetQuery(connexion, "SELECT * FROM sites ORDER BY ph LIMIT 5")
```

```
  id nom       ph
1 21   U 6.577113
2 24   X 6.655040
3  8   H 6.692542
4 20   T 6.708648
5  4   D 6.746626
```

La clause LIMIT deviendra particulièrement importante dans les requêtes à
venir, qui pourraient retourner des millions de lignes...

### Connecter plusieurs tables ensemble
Puisque nos données sont bien organisées en différentes tables avec une
structure normalisée, on peut facilement exploiter les relations entre les
différentes tables.

Comme premier scénario, imaginons que l'on voudrait obtenir toutes les parcelles
situées dans des sites au pH de 7 ou plus.

Première étape, préparer une requête pour trouver ces sites : 

```r
dbGetQuery(connexion, "SELECT * FROM sites WHERE ph >= 7")
```

```
   id nom       ph
1   1   A 7.169245
2   5   E 7.241875
3   9   I 7.213349
4  10   J 7.310457
5  12   L 7.693323
6  13   M 7.358573
7  14   N 7.196335
8  16   P 7.118721
9  18   R 7.125312
10 19   S 7.078200
11 22   V 7.569592
```

Pour se faciliter la vie, nous allons introduire ici le concept d'alias, qui nous 
permet de renommer une table dans notre requête pour y faire référence plus facilement.
Notez que les alias n'existent qu'au moment de cette requête en particulier. Lors de 
la prochaine, nous devrons le refaire cet alias si on veut l'utiliser à nouveau.

Nous en profiterons aussi pour clarifier que le pH et le * qui nous intéressent
sont ceux de la table de sites : 


```r
dbGetQuery(connexion, "SELECT s.* FROM sites AS s WHERE s.ph >= 7")
```

```
   id nom       ph
1   1   A 7.169245
2   5   E 7.241875
3   9   I 7.213349
4  10   J 7.310457
5  12   L 7.693323
6  13   M 7.358573
7  14   N 7.196335
8  16   P 7.118721
9  18   R 7.125312
10 19   S 7.078200
11 22   V 7.569592
```

Maintenant, on peut aller connecter cette requête avec la table de parcelles : 

```r
dbGetQuery(connexion, "
           SELECT s.*, p.* 
           FROM sites AS s 
           LEFT JOIN parcelles AS p
           ON p.site_id = s.id
           WHERE s.ph >= 7
           LIMIT 10
          ")
```

```
   id nom       ph     id site_id        nom couvert_pct
1   5   E 7.241875 122881       5 megvgtlavc   60.040527
2   9   I 7.213349 122885       9 vnamlyqykq   10.955212
3  10   J 7.310457 122886      10 voynebxemt   87.516474
4  12   L 7.693323 122888      12 dcwdppalwt   91.731660
5  13   M 7.358573 122889      13 wdphqdvvnt   23.368555
6  14   N 7.196335 122890      14 vcvtjgthjp    3.024725
7  16   P 7.118721 122892      16 pjskhveidz   71.662687
8  18   R 7.125312 122894      18 lexdywvieo   91.496724
9  19   S 7.078200 122895      19 jlvqsnbyvy   61.266997
10 22   V 7.569592 122898      22 kfamkmimao   74.941283
```

Remarquez que l'on a besoin de deux clauses pour connecter les deux tables 
ensemble. 

La première est le LEFT JOIN. Il existe différents types de "jointures"
en SQL, la "gauche" étant la plus commune. Dans un LEFT JOIN, toutes les 
données de la table de gauche (ici les sites) seront présentes, et seules
les données de la table de droite ayant des correspondances seront ajoutées.

Comme le SQL ne lit pas dans les pensées, il faut aussi ajouter une clause ON, 
qui spécifie sur quels champs connecter les deux tables. Ici, on 
spécifie que la colonne site_id de la table parcelles doit correspondre
à la colonne id de la table de sites.

J'ai utilisé ici la clause LIMIT 10 pour éviter d'avoir 100 pages de résultat,
mais dans une utilisation réelle, ce n'aurait pas été nécessaire.

Une chose particulièrement contre-intuitive ici est que l'on peut utiliser l'information
provenant de la parcelle dans la clause SELECT, même si à ce point, nous n'avons pas
mentionné que l'on voulait s'y lier. Le code SQL n'est pas interprété
de façon séquentielle. La requête sera déconstruite et interprétée de façon optimale,
pour l'ensemble de la requête.

Remarquez aussi que l'on mentionne que l'on veut toute l'info provenant des deux
tables (s.*, p.*), mais ce n'est pas obligatoire. On aurait aussi pu être
sélectifs : 

```r
dbGetQuery(connexion, "
           SELECT s.nom AS nom_site, s.ph, p.nom AS nom_parcelle, p.couvert_pct
           FROM sites AS s 
           LEFT JOIN parcelles AS p
           ON p.site_id = s.id
           WHERE s.ph >= 7
           LIMIT 10
          ")
```

```
   nom_site       ph nom_parcelle couvert_pct
1         A 7.169245   yjsvyluwyw    13.35865
2         E 7.241875   dvbqleheht    42.34773
3         I 7.213349   dspjaogwaf    53.68863
4         J 7.310457   wenvtalrhm    33.90705
5         L 7.693323   yiufghvqze    54.29557
6         M 7.358573   segjczoozw    46.62623
7         N 7.196335   bkbedboyii    97.87763
8         P 7.118721   esqsvztcos    13.13683
9         R 7.125312   vzozqzgsnw    19.40764
10        S 7.078200   xnvlnblopy    42.59576
```

Remarquez ici que l'on peut utiliser des alias dans la clause SELECT de notre
requête

### Effectuer des opérations par groupe

Le SQL contient aussi toute une série de mots clés permettant de faire des opérations
par groupe.

Sachant que COUNT nous fournit le nombre de lignes correspondant dans une requête, p.ex.

```r
dbGetQuery(connexion,"SELECT COUNT(id) FROM sites")
```

```
  count(id)
1        26
```

On peut, par exemple, se demander combien de fois chacune des espèces
a été observée : 

```r
dbGetQuery(connexion,"
           SELECT COUNT(p.espece_id)
           FROM presences AS p
           GROUP BY p.espece_id
           LIMIT 10
           ")
```

```
   count(p.espece_id)
1               75840
2               22797
3              169707
4              160021
5              198948
6              177248
7              118475
8               87996
9              175962
10              27476
```

Évidemment, cette façon de présenter l'information n'est pas très utile, mais
peut facilement le devenir en connectant la table d'espèces : 

```r
dbGetQuery(connexion,"
           SELECT COUNT(p.espece_id) AS nb_observations, ANY_VALUE(e.nom) AS nom
           FROM presences AS p
           LEFT JOIN especes AS e ON e.id = p.espece_id
           GROUP BY e.id
           ORDER BY nb_observations DESC
           LIMIT 10
           ")
```

```
   nb_observations                            nom
1           439378      African giant pouched rat
2           439314 Thirteen-lined ground squirrel
3           436888               Little brown bat
4           434872                     Musk shrew
5           413442                        Giraffe
6           412510          African striped mouse
7           409380           Round-tailed muskrat
8           408834               African elephant
9           402402         Arctic ground squirrel
10          397896                            Cow
```

Remarquez que l'on a dû ajouter la clause ANY_VALUE autour de e.nom. Lorsque
l'on applique une opération par groupe, toutes les valeurs de la clause
SELECT doivent subir une opération de type agrégation (i.e. COUNT, MIN,
MAX,AVG, FIRST, LAST, ANY_VALUE, etc.) ou faire partie de la cause GROUP BY.

### Vues et sous-requêtes

Enfin, comme dernier exemple d’agrégation, nous allons calculer la richesse
en espèce moyenne par parcelle pour chaque site. Ce sera notre requête la
plus complexe de l'atelier!

Tout d'abord, construisons la requête qui calcule le nombre d'espèce par
parcelle : 

```r
dbGetQuery(connexion,"
  SELECT 
    ANY_VALUE(pa.nom) AS nom,
    ANY_VALUE(pa.id) AS id, 
    ANY_VALUE(pa.site_id) AS site_id, 
    COUNT(DISTINCT(pr.espece_id)) AS richesse
  FROM parcelles AS pa
  LEFT JOIN presences as pr ON pr.parcelle_id = pa.id
  GROUP BY pa.id
  LIMIT 10
           ")
```

```
          nom      id site_id richesse
1  nldjabkezk 1264584      22        8
2  wanjivedhu 1272225      19       14
3  ggswzbuius   72442       6       21
4  xbjkkbamow  114094       6        6
5  gxdsdbdicm 1287497       3       10
6  dokcidjdqi  814664       6        4
7  zskfrdbata  145840       6       16
8  fccafsyimm 1274512      18       11
9  owgxsxcczv  467450      22       10
10 ymmbyupkzm  186856      20       14
```

Ensuite, il est extrêmement pratique de savoir qu'il existe dans les bases de données
des objets que l'on nomme des vues. Ces dernières sont une façon de sauvegarder une requête
et de lui associer un nom, pour la réutiliser plus tard.

On peut créer une vue à partir de la requête ci-haut grâce à la commande CREATE VIEW :

```r
dbExecute(connexion,"
  CREATE VIEW v_richesses AS
  SELECT 
    ANY_VALUE(pa.nom) AS nom,
    ANY_VALUE(pa.id) AS id, 
    ANY_VALUE(pa.site_id) AS site_id, 
    COUNT(DISTINCT(pr.espece_id)) AS richesse
    FROM parcelles AS pa
    LEFT JOIN presences as pr ON pr.parcelle_id = pa.id
    GROUP BY pa.id")
```

```
[1] 0
```

Cette vue peut ensuite être utilisée exactement comme si il s'agissait d'une table : 

```r
dbGetQuery(connexion, "SELECT * FROM v_richesses LIMIT 5")
```

```
         nom      id site_id richesse
1 nolxmnqmdx 1058277      25       11
2 cwbbnvcniq 1285844      14        4
3 fzlzionnwq  846829       9       12
4 xiewxpzcrz  635508      16       11
5 jorditqaqa 1042151      19       12
```

Pour calculer la richesse moyenne par site, il ne nous reste maintenant
qu'à connecter les deux tables ensemble à l'aide d'une jointure : 

```r
  dbGetQuery(connexion, "
    SELECT sites.id, ANY_VALUE(sites.nom) AS nom, ANY_VALUE(sites.ph) AS ph, MEAN(richesse) AS richesse_moyenne
    FROM sites
    LEFT JOIN v_richesses
    ON v_richesses.site_id = sites.id
    GROUP BY sites.id
    LIMIT 5
  ")
```

```
  id nom       ph richesse_moyenne
1  1   A 7.169245          7.12628
2  2   B 6.954659          7.13368
3  3   C 6.798699          7.09738
4  4   D 6.746626          7.13422
5  5   E 7.241875          7.11826
```

Remarquez qu'une stratégie alternative aurait été d'utiliser une sous-requête 
pour arriver au même résultat, mais de façon beaucoup moins lisible : 


```r
dbGetQuery(connexion,"
  SELECT ANY_VALUE(s.nom) AS nom, ANY_VALUE(s.ph) AS ph, AVG(p.richesse) AS richesse_moyenne
  FROM sites AS s
  LEFT JOIN (
    SELECT 
    ANY_VALUE(pa.nom) AS nom,
    ANY_VALUE(pa.id) AS id, 
    ANY_VALUE(pa.site_id) AS site_id, 
    COUNT(DISTINCT(pr.espece_id)) AS richesse
    FROM parcelles AS pa
    LEFT JOIN presences as pr ON pr.parcelle_id = pa.id
    GROUP BY pa.id
  ) AS p ON p.site_id = s.id
  GROUP BY s.id
  ORDER BY richesse_moyenne DESC
           ")
```

```
   nom       ph richesse_moyenne
1    L 7.693323          7.15808
2    X 6.655040          7.15736
3    K 6.945442          7.14678
4    Q 6.759258          7.13844
5    R 7.125312          7.13674
6    D 6.746626          7.13422
7    B 6.954659          7.13368
8    M 7.358573          7.13298
9    V 7.569592          7.13278
10   I 7.213349          7.12652
11   A 7.169245          7.12628
12   F 6.970178          7.12502
13   Z 6.876635          7.12202
14   E 7.241875          7.11826
15   U 6.577113          7.11738
16   O 6.865845          7.11498
17   N 7.196335          7.10808
18   G 6.828074          7.10546
19   W 6.879513          7.09800
20   C 6.798699          7.09738
21   S 7.078200          7.09358
22   H 6.692542          7.08934
23   T 6.708648          7.08824
24   Y 6.991663          7.07886
25   J 7.310457          7.07566
26   P 7.118721          7.06196
```

## Insertion, modification et suppression de données
Dans le cadre de votre travail de biologiste, vous utiliserez probablement 
une base de données relativement statique, créée comme nous l'avons montré
plus haut, directement à partir de fichiers CSV.

Néanmoins, il est intéressant de connaître la capacités complètes d'un
système de base de données relationnelle. En particulier, comment ajouter,
modifier et supprimer des données.

### Insertion de données

Comme nous en avons discuté précédemment, il est très important dans une
base de données relationnelle que chaque ligne dans une table puisse
être identifiée de façon unique.

Certaines bases de données comme MySQL permettent, au moment de créer une 
table, de mentionner qu'un identifiant unique devra être généré chaque
fois qu'une ligne est ajoutée.

Avec DuckDB, il faudra travailler un petit peu plus pour arriver au même
résultat. Avant d'effectuer la première insertion, nous devrons créer
un objet de type séquence, qui nous permettra d'obtenir la prochaine
valeur d'id à utiliser lors de notre insertion.

Pour cette section, nous enverrons nos commandes, non pas à l'aide de la 
fonction dbGetQuery, mais à l'aide de la fonction dbExecute, puisque
nos commandes SQL ne sont pas des requêtes conçues pour retourner des données.

Donc, voyons d'abord comment créer une séquence pour la table de sites, en 
spécifiant que la première valeur de la séquence devra être l'id maximum de
site + 1.

Remarquez que contrairement aux id qui *s'auto-incrémentent*, cette stratégie
est plutôt risquée si plusieurs personnes travaillent simultanément sur la même
base de données. Après avoir récupéré l'id maximal, une nouvelle insertion pourrait
avoir lieu et changer ce maximum. De plus, il serait possible pour quelqu'un
d'insérer une ligne sans utiliser la séquence, etc.

Donc, premièrement, quel est le plus grand id de notre table de sites?

```r
dbGetQuery(connexion,"SELECT MAX(id) FROM sites")
```

```
  max(id)
1      26
```

Deuxièmement, on crée la séquence, en réutilisant ce chiffre

```r
dbExecute(connexion, "CREATE SEQUENCE sequence_sites START 27")
```

```
[1] 0
```

On peut donc ensuite utiliser cette séquence pour les insertions subséquentes : 

```r
dbExecute(connexion, "INSERT INTO sites VALUES (nextval('sequence_sites'), 'site ajouté', 7.0)")
```

```
[1] 1
```

```r
dbExecute(connexion, "INSERT INTO sites VALUES (nextval('sequence_sites'), 'deuxième ajout', 6.5)")
```

```
[1] 1
```

On peut maintenant aller voir dans la table que nos sites ont effectivement été
ajoutés, avec des identifiants uniques : 

```r
dbGetQuery(connexion,"SELECT * FROM sites ORDER BY id DESC LIMIT 10")
```

```
   id            nom       ph
1  28 deuxième ajout 6.500000
2  27    site ajouté 7.000000
3  26              Z 6.876635
4  25              Y 6.991663
5  24              X 6.655040
6  23              W 6.879513
7  22              V 7.569592
8  21              U 6.577113
9  20              T 6.708648
10 19              S 7.078200
```

Sachant cela, il pourrait être tentant d'insérer des données en utilisant
des objets existants dans R, par exemple comme ceci : 

```r
nom <- "Site de Charles"
pH <- 7

dbExecute(connexion, paste0(
  "INSERT INTO sites VALUES (nextval('sequence_sites'), '",
  nom,
  "', ",
  pH,
  ")"
))
```

```
[1] 1
```

Remarquez d'abord que ce genre d'opération nécessite *beaucoup* de concentration,
pour ne pas mélanger les guillemets doubles qui définissent nos chaînes de 
caractères dans R, et les guillemets simples, qui définissent les chaînes
de caractère en SQL.

Remarquez aussi que, si notre nom de site contient, entre autres, une apostrophe,
nous aurons une erreur SQL : 

```r
nom <- "L'autre site"
pH <- 7

dbExecute(connexion, paste0(
  "INSERT INTO sites VALUES (nextval('sequence_sites'), '",
  nom,
  "', ",
  pH,
  ")"
))
```

```
Error: Parser Error: syntax error at or near "autre"
LINE 1: ... VALUES (nextval('sequence_sites'), 'L'autre site', 7)
                                                  ^
```

Ce genre de problème a donné lieu a beaucoup de cas de piratage de sites
web au début des années 1990. Il suffit par exemple d'imaginer un 
formulaire dans lequel un visiteur mal intentionné entrerait 
`A'; DELETE * FROM users;` ou
`A';INSERT INTO admin VALUES ''pirate'',''motdepassedupirate'';` comme nom d'usager
ou mot de passe !!!

Pour ces deux raisons, il est maintenant recommandé de toujours utiliser
les fonctions de protection fournies avec la librairie qui gère votre
base de données. Dans notre cas, on parle de la combinaison DBI + DuckDB. L'idée
derrière ces fonctions de protection sera d'encoder tous les éléments potentiellement
dangereux d'une requête pour éviter ces problèmes, qu'ils soient accidentels
ou malveillants.

Pour cela, il faudra travailler en deux étapes, soit préparer le *squelette*
de notre requête, et ensuite, dans un deuxième temps, demander à DBI 
d'effectuer les remplacements.

Dans le squelette de requête, tous les éléments qui proviendront de l'extérieur
sont précédés d'un `?` : 


```r
valeur_tannante <- "L'apostrophe de malheur"
squelette <- "INSERT INTO sites VALUES (nextval('sequence_sites'),?nom,?ph)"
```

Ensuite, la fonction interpolate nous permet de construire une requête,
où on fournit les valeurs à utiliser pour chacun des remplacements

```r
requete <- sqlInterpolate(
  connexion,
  squelette,
  nom="L'autre site",
  ph=6.5
)
print(requete)
```

```
<SQL> INSERT INTO sites VALUES (nextval('sequence_sites'),'L''autre site',6.5)
```

```r
dbExecute(connexion,requete)
```

```
[1] 1
```

Cette façon de faire devrait être appliquée chaque fois qu'une partie de la requête
provient d'un objet de R, même lorsqu'il s'agit d'un simple SELECT : 

```r
squelette <- "SELECT * FROM sites WHERE nom = ?nom"
requete <- sqlInterpolate(
  connexion,
  squelette,
  nom="L'autre site"
)
print(requete)
```

```
<SQL> SELECT * FROM sites WHERE nom = 'L''autre site'
```

```r
dbGetQuery(connexion, requete)
```

```
  id          nom  ph
1 30 L'autre site 6.5
```

### Suppression de données
La suppression de données en SQL s'effectue avec la fonction... DELETE!

Vérifions d'abord que nous avons des sites avec des pH sous 6.6 :

```r
dbGetQuery(connexion, "SELECT * FROM sites ORDER BY ph LIMIT 5")
```

```
  id            nom       ph
1 28 deuxième ajout 6.500000
2 30   L'autre site 6.500000
3 21              U 6.577113
4 24              X 6.655040
5  8              H 6.692542
```

Pour supprimer tous les sites dont le pH est < 6.6, on pourrait
lancer la requête suivante : 

```r
dbExecute(connexion,"DELETE FROM sites WHERE ph < 6.6")
```

```
[1] 3
```

On peut ensuite vérifier que nos données ont bien été supprimées : 

```r
dbGetQuery(connexion, "SELECT * FROM sites ORDER BY ph LIMIT 5")
```

```
  id nom       ph
1 24   X 6.655040
2  8   H 6.692542
3 20   T 6.708648
4  4   D 6.746626
5 17   Q 6.759258
```

Dans les bases de données plus avancées comme MySQL, PostgreSQL, etc.,
une telle opération aurait aussi pu entraîner la suppression de toutes
les parcelles associées au site, de toutes les présences associées aux
parcelles, etc. Par contre, comme DuckDB est une base de données locale,
et légère conçue pour une seul utilisateur à la fois, cette cascade n'a
pas eu lieu : 

```r
dbGetQuery(connexion, "SELECT * FROM parcelles WHERE site_id = 21 LIMIT 5")
```

```
   id site_id        nom couvert_pct
1  21      21 otuzagswlo    55.11549
2  47      21 orqxestuqr    58.59047
3  73      21 vytwvjjvcr    47.13821
4  99      21 odicuudsgl    42.04843
5 125      21 izpwayhfaq    78.15716
```

## Mise à jour des données
Enfin, on peut mettre à jour des données avec la commande UPDATE.

Par exemple, si on s'est trompés et que toutes les observations de la parcelle
3 étaient en fait la parcelle 4, on pourrait faire :

```r
dbExecute(connexion,"UPDATE presences SET parcelle_id = 4 WHERE parcelle_id = 3")
```

```
[1] 7
```

Et ensuite vérifier que tout s'est bien déroulé : 


```r
dbGetQuery(connexion, "
           SELECT parcelle_id, COUNT(*) 
           FROM presences 
           WHERE parcelle_id = 3 OR parcelle_id = 4
           GROUP BY parcelle_id
          ")
```

```
  parcelle_id count_star()
1           4           10
```


## Conclusion et références
Comme pour la majorité des ateliers du Numérilab, nous n'avons ici qu'effleuré la surface de ce qu'il est possible de faire avec le SQL. J'espère que vous avez apprécié le fait que notre base de données traitait des requêtes sur des millions
de lignes sans même sourciller. Évidemment, les bases de données relationnelles et le SQL ont leur place dans 
différents systèmes informatiques, allant de quelques Mo de données pour un projet de maîtrise, à plusieurs
Po de données pour gérer le contenu d'un site web transactionnel de classe mondiale.

Si vous voulez en apprendre plus sur le SQL, la ressources que je vous recommanderais probablement (et celle qui m'a aidé à me dérouiller après quelques années sans faire de SQL) est le tutoriel de W3Schools : 
https://www.w3schools.com/sql/.

Je vous encourage aussi fortement à aller explorer SQLite, une autre base de données légère et locale comme DuckDB, 
qui est utilisée comme stockage dans beaucoup d'applications.
